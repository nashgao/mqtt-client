---
allowed-tools: all
description: **EXECUTE test coverage analysis** and implement comprehensive testing for critical code paths
intensity: ‚ö°‚ö°‚ö°
pattern: üìäüìäüìä
---

# üìäüìäüìä CRITICAL TEST COVERAGE EXECUTION: COMPREHENSIVE COVERAGE IMPROVEMENT! üìäüìäüìä

**THIS IS NOT A SIMPLE COVERAGE REPORT - THIS IS A COMPREHENSIVE COVERAGE EXECUTION SYSTEM!**

**üö® ACTUAL TEST EXECUTION AND COVERAGE MEASUREMENT REQUIRED! üö®**

## üö® ZERO TOLERANCE ENFORCEMENT

**MANDATORY - ALL tests must achieve PERFECT execution:**

### Success Criteria (ALL must be met)
- ‚úÖ **0 Failed Tests** - Every single test must pass
- ‚úÖ **0 Errors** - No runtime errors allowed
- ‚úÖ **0 Warnings** - Warnings are treated as failures
- ‚úÖ **0 Deprecations** - Deprecation notices block success
- ‚úÖ **0 Incomplete Tests** - Incomplete tests are failures
- ‚úÖ **0 Risky Tests** - Risky test detection must pass
- ‚úÖ **0 Skipped Tests** - Unless explicitly allowed with justification

### Failure Response Protocol
When ANY issue is detected:
1. **STOP** - Do not proceed to next steps
2. **REPORT** - List all issues with file:line references
3. **FIX** - Resolve ALL issues before continuing
4. **VERIFY** - Re-run tests to confirm 100% clean execution

### Exit Codes
- `0` = Perfect execution (no warnings, no deprecations, no failures)
- `1` = Any failure, warning, deprecation, or incomplete test
- `2` = Configuration or setup error

When you run `/test coverage`, you are REQUIRED to:

1. **EXECUTE** tests to measure current coverage and identify critical gaps
2. **IMPLEMENT** comprehensive tests for all uncovered critical paths
3. **PRIORITIZE** coverage by business impact and code complexity
4. **USE MULTIPLE AGENTS** for parallel coverage improvement:
   - Spawn one agent per module or critical component
   - Spawn agents for different test types (unit, integration, edge cases)
   - Say: "I'll spawn multiple agents to improve test coverage across all critical code paths"
5. **GENERATE** detailed coverage reports with actionable recommendations
6. **ACHIEVE** coverage targets while maintaining test quality

## ‚ö†Ô∏è SPECIFICATION-FIRST PHILOSOPHY (MANDATORY)

**Coverage analysis must identify UNSPECIFIED BEHAVIORS, not just uncovered code paths.**

- Coverage gaps mean: "Which consumer-facing behaviors lack test specifications?"
- Do NOT write tests by reading uncovered code and adding assertions for what it does
- DO write tests by identifying what behaviors consumers expect but have no specification
- See `templates/CLAUDE.md` ‚Üí "MANDATORY: Specification-First Testing" for full mandate

**Key Distinctions:**
- ‚ùå **Confirmation testing**: Read code ‚Üí write tests that verify what code does ‚Üí tests pass by construction
- ‚úÖ **Specification testing**: Define expected behavior ‚Üí write test ‚Üí implement code to make it pass

**Coverage-First Workflow:**
1. Identify uncovered code paths (as indicators, not goals)
2. Ask: "What consumer-facing behavior is this code supposed to implement?"
3. Write test specifications for those behaviors WITHOUT reading the implementation
4. Verify tests fail when behavior is broken in real consumer usage
5. Only then use coverage tools to validate comprehensive behavior coverage

## üéØ USE MULTIPLE AGENTS

**MANDATORY TASK TOOL AGENT SPAWNING:**

### Coverage Discovery Agent:
```markdown
<function_calls>
<invoke name="Task">
<parameter name="subagent_type">test-fixer</parameter>
<parameter name="description">Discover coverage setup</parameter>
<parameter name="prompt">You are the Coverage Discovery Agent for test coverage analysis.

Your responsibilities:
1. Scan project for test frameworks and coverage tools
2. Identify existing test files and coverage configuration
3. Set up coverage tooling if missing
4. Configure coverage thresholds and targets
5. Generate discovery report with framework detection

MANDATORY RESULT TRACKING:
- You MUST save your analysis results to /tmp/test-coverage-discovery-results.json
- Include success: true/false field in your JSON output
- Document framework detected, existing coverage setup, and configuration status
- Report any setup failures or missing tools

Execute discovery commands based on detected framework:
- Jest: Check package.json for Jest config and coverage setup
- pytest: Check for .coveragerc, pyproject.toml, or setup.cfg
- Go: Check for coverage tools and test files with -cover flags
- PHPUnit: Check phpunit.xml for coverage configuration

CRITICAL: Your result JSON must include actual coverage tool availability and setup status.</parameter>
</invoke>
</function_calls>
```

### Coverage Analysis Agent:
```markdown
<function_calls>
<invoke name="Task">
<parameter name="subagent_type">test-fixer</parameter>
<parameter name="description">Analyze coverage</parameter>
<parameter name="prompt">You are the Coverage Analysis Agent for test coverage measurement.

Your responsibilities:
1. Run existing tests to generate coverage data
2. Identify which consumer-facing behaviors lack test specifications
3. Prioritize unspecified behaviors by business impact
4. Map untested consumer expectations and use cases
5. Generate comprehensive specification gap analysis report

CRITICAL: Coverage gaps indicate MISSING SPECIFICATIONS, not just uncovered lines.
Ask: "What behaviors do consumers expect that have no test specification?"

MANDATORY TEST EXECUTION COMMANDS:
You MUST actually run coverage tests, not just analyze existing reports:

Jest projects: npx jest --coverage --json --outputFile=coverage-summary.json
pytest projects: python -m pytest --cov=. --cov-report=json:coverage.json --cov-report=html
Go projects: go test -coverprofile=coverage.out -json ./... && go tool cover -func=coverage.out
PHPUnit projects: ./vendor/bin/phpunit --coverage-clover coverage.xml --coverage-html coverage-html

MANDATORY RESULT TRACKING:
- You MUST save analysis results to /tmp/test-coverage-analysis-results.json
- Include success: true/false, current_coverage, gaps_identified, and priority_areas
- Document actual coverage percentages from test execution
- Report any test execution failures or coverage generation issues

CRITICAL: You must execute coverage tests to get real coverage data.</parameter>
</invoke>
</function_calls>
```

### Gap Identification Agent:
```markdown
<function_calls>
<invoke name="Task">
<parameter name="subagent_type">test-fixer</parameter>
<parameter name="description">Identify gaps</parameter>
<parameter name="prompt">You are the Gap Identification Agent for coverage gap analysis.

Your responsibilities:
1. Read coverage data from analysis results
2. Identify critical consumer-facing behaviors without test specifications
3. Find unspecified edge cases and error scenarios from consumer perspective
4. Prioritize specification gaps by consumer impact and risk
5. Generate prioritized specification gap list with actionable recommendations

CRITICAL: Gaps are UNSPECIFIED BEHAVIORS, not uncovered code lines.
Focus on: "What should this feature do for consumers?" not "What does this code do?"

MANDATORY RESULT TRACKING:
- You MUST save gap analysis results to /tmp/test-coverage-gaps-results.json
- Include success: true/false field based on gap identification completion
- Document critical_gaps, high_priority_gaps, and estimated_effort
- Only execute after Coverage Analysis Agent confirms results

CRITICAL: Wait for coverage analysis results before identifying gaps.</parameter>
</invoke>
</function_calls>
```

### Test Creation Agent:
```markdown
<function_calls>
<invoke name="Task">
<parameter name="subagent_type">test-fixer</parameter>
<parameter name="description">Create tests</parameter>
<parameter name="prompt">You are the Test Creation Agent for coverage improvement.

Your responsibilities:
1. Read prioritized specification gaps from gap identification results
2. Create test specifications for unspecified consumer behaviors
3. Write tests that define expected behavior BEFORE reading implementation
4. Implement tests for consumer-facing edge cases and error scenarios
5. Ensure all new tests specify real behavior consumers depend on

MANDATORY SPECIFICATION-FIRST TEST CREATION:
You MUST write tests as specifications, NOT confirmations:
- Write test specifications BEFORE looking at implementation code
- Define expected consumer-facing behavior in test names and assertions
- Verify tests would FAIL if the feature was broken in real usage
- Write from consumer perspective: "When I do X, I expect Y"
- Measure specification coverage improvement, not just line coverage

MANDATORY RESULT TRACKING:
- You MUST save creation results to /tmp/test-coverage-creation-results.json
- Include success: true/false, tests_created, coverage_improvement, and test_files
- Document actual test files created and their execution results
- Only execute after Gap Identification Agent confirms results

CRITICAL: You must create and execute real test files, not just generate plans.</parameter>
</invoke>
</function_calls>
```

### Report Generation Agent:
```markdown
<function_calls>
<invoke name="Task">
<parameter name="subagent_type">general-purpose</parameter>
<parameter name="description">Generate reports</parameter>
<parameter name="prompt">You are the Report Generation Agent for coverage reporting.

Your responsibilities:
1. Aggregate coverage metrics from all test runs
2. Generate comprehensive coverage reports
3. Create actionable improvement recommendations
4. Visualize coverage trends and gaps
5. Output final comprehensive coverage report

MANDATORY RESULT AGGREGATION:
- Aggregate results from /tmp/test-coverage-*-results.json files
- Validate all agents completed successfully
- Create unified coverage improvement report
- Generate before/after coverage comparison
- Document improvement recommendations

MANDATORY RESULT TRACKING:
- You MUST save report results to /tmp/test-coverage-report-results.json
- Include success: true/false based on report generation completion
- Document final coverage metrics, improvements achieved, and next steps
- Report any aggregation failures or missing agent results

CRITICAL: Wait for all coverage agents to complete before generating final report.</parameter>
</invoke>
</function_calls>
```

## üö® FORBIDDEN BEHAVIORS

**NEVER:**
- ‚ùå **Write tests by reading uncovered source code and confirming what it does** ‚Üí NO! Write specifications of expected behavior!
- ‚ùå **Treat line coverage as the primary metric** ‚Üí NO! Specification coverage matters more than line coverage!
- ‚ùå **"This code is uncovered, let me read it and write a test"** ‚Üí NO! Ask "What behavior should this implement?"
- ‚ùå "85% coverage is good enough" ‚Üí NO! Focus on critical path coverage quality!
- ‚ùå Skip complex functions ‚Üí NO! Complex code needs more testing!
- ‚ùå Write tests just for coverage numbers ‚Üí NO! Tests must be meaningful!
- ‚ùå Ignore error paths ‚Üí NO! Error handling must be tested!
- ‚ùå Skip edge cases ‚Üí NO! Boundary conditions are critical!
- ‚ùå "Coverage tools are inaccurate" ‚Üí NO! Use multiple coverage metrics!

**MANDATORY WORKFLOW:**
```
1. TEST EXECUTION ‚Üí Run tests to generate coverage data
2. Specification gap analysis ‚Üí Identify unspecified consumer behaviors (coverage is indicator, not goal)
3. IMMEDIATELY spawn 5 agents for parallel specification creation
4. AGENT RESULT VERIFICATION ‚Üí Validate all agents completed successfully
5. Consumer behavior specification ‚Üí Define expected behaviors for critical paths
6. Specification-first test writing ‚Üí Write tests WITHOUT reading implementation
7. VERIFICATION EXECUTION ‚Üí Re-run tests to verify behavior specifications pass
8. FINAL SUCCESS VALIDATION ‚Üí Validate consumer behaviors are fully specified
```

## AGENT RESULT VERIFICATION (MANDATORY)

After spawning all 5 coverage agents, you MUST verify their results:

```bash
# MANDATORY: Verify all agents completed successfully
AGENT_RESULTS_DIR="/tmp"
AGENT_FILES=("test-coverage-discovery-results.json" "test-coverage-analysis-results.json" "test-coverage-gaps-results.json" "test-coverage-creation-results.json" "test-coverage-report-results.json")

for result_file in "${AGENT_FILES[@]}"; do
    FULL_PATH="$AGENT_RESULTS_DIR/$result_file"
    if [ -f "$FULL_PATH" ]; then
        # Use jq to parse agent results
        AGENT_SUCCESS=$(jq -r '.success // false' "$FULL_PATH" 2>/dev/null || echo 'false')
        if [ "$AGENT_SUCCESS" != "true" ]; then
            echo "‚ùå CRITICAL: Coverage agent failed to complete successfully"
            echo "   Failed agent result: $result_file"
            echo "   Check agent logs for failure details"
            exit 1
        fi
    else
        echo "‚ùå CRITICAL: Missing coverage agent result file: $result_file"
        echo "   Agent may have failed to complete or save results"
        exit 1
    fi
done

echo "‚úÖ All coverage agents completed successfully"
```

## FRAMEWORK-SPECIFIC COVERAGE EXECUTION (MANDATORY)

After agent coordination, you MUST execute actual coverage tests:

```bash
# Detect framework and run appropriate coverage tests
if [ -f "package.json" ] && grep -q "jest\|mocha\|vitest" package.json; then
    echo "üìä Executing Jest/Node.js coverage tests..."
    npx jest --coverage --json --outputFile=coverage-summary.json
    COVERAGE_EXIT_CODE=$?
elif [ -f "requirements.txt" ] || [ -f "setup.py" ] || [ -f "pyproject.toml" ]; then
    echo "üìä Executing pytest coverage tests..."
    python -m pytest --cov=. --cov-report=json:coverage.json --cov-report=html --cov-report=term
    COVERAGE_EXIT_CODE=$?
elif ls *.go 1> /dev/null 2>&1; then
    echo "üìä Executing Go coverage tests..."
    go test -coverprofile=coverage.out -json ./... && go tool cover -func=coverage.out
    COVERAGE_EXIT_CODE=$?
elif [ -f "composer.json" ] && [ -d "vendor/phpunit" ]; then
    echo "üìä Executing PHPUnit coverage tests..."
    ./vendor/bin/phpunit --coverage-clover coverage.xml --coverage-html coverage-html --coverage-text
    COVERAGE_EXIT_CODE=$?
elif [ -f "Gemfile" ] && grep -q "rspec" Gemfile; then
    echo "üìä Executing RSpec coverage tests..."
    bundle exec rspec --require simplecov
    COVERAGE_EXIT_CODE=$?
else
    echo "‚ùå No supported test framework detected for coverage"
    exit 1
fi

# MANDATORY: Validate coverage test execution success
if [ $COVERAGE_EXIT_CODE -ne 0 ]; then
    echo "‚ùå CRITICAL: Coverage tests failed with exit code $COVERAGE_EXIT_CODE"
    echo "   Coverage analysis cannot proceed with failing tests"
    echo "   Fix test failures before measuring coverage"
    exit $COVERAGE_EXIT_CODE
fi

echo "‚úÖ Coverage tests executed successfully"

# Extract coverage percentage (framework-specific parsing)
COVERAGE_PERCENTAGE="unknown"
if [ -f "coverage-summary.json" ]; then
    # Jest coverage
    COVERAGE_PERCENTAGE=$(jq -r '.total.lines.pct // "unknown"' coverage-summary.json)
elif [ -f "coverage.json" ]; then
    # pytest coverage
    COVERAGE_PERCENTAGE=$(jq -r '.totals.percent_covered // "unknown"' coverage.json)
elif [ -f "coverage.out" ]; then
    # Go coverage
    COVERAGE_PERCENTAGE=$(go tool cover -func=coverage.out | tail -1 | awk '{print $3}' | sed 's/%//')
fi

echo "üìä Current coverage: $COVERAGE_PERCENTAGE%"
```

**YOU ARE NOT DONE UNTIL:**
- ‚úÖ ALL critical consumer-facing behaviors have test specifications
- ‚úÖ Specification coverage is achieved (tests define expected behavior, not confirm existing code)
- ‚úÖ Consumer error scenarios and edge cases have behavior specifications
- ‚úÖ Coverage reports identify unspecified behaviors, not just uncovered lines
- ‚úÖ Test quality ensures tests would fail if behavior breaks in real usage
- ‚úÖ Performance-critical consumer paths have benchmark specifications

---

üõë **MANDATORY COVERAGE IMPROVEMENT CHECK** üõë
1. Re-read ~/.claude/CLAUDE.md RIGHT NOW
2. Check current codebase and existing test coverage
3. Verify you understand the coverage improvement requirements

Execute comprehensive coverage improvement for: $ARGUMENTS

**FORBIDDEN SHORTCUT PATTERNS:**
- "Let me read this uncovered code and write a test" ‚Üí NO, specify expected behavior first!
- "90% line coverage means we're done" ‚Üí NO, do all consumer behaviors have specifications?
- "This code is too simple to test" ‚Üí NO, all consumer behaviors need specifications!
- "Integration tests cover this" ‚Üí NO, does each test specify a consumer expectation?
- "Coverage numbers look good" ‚Üí NO, focus on specification coverage, not line coverage!
- "Edge cases are unlikely" ‚Üí NO, specify all consumer-facing boundary behaviors!
- "Performance tests are optional" ‚Üí NO, specify performance expectations for critical paths!

Let me ultrathink about the comprehensive coverage improvement architecture and strategy.

üö® **REMEMBER: Quality coverage prevents bugs and improves maintainability!** üö®

**Comprehensive Coverage Improvement Protocol:**

**Step 0: Coverage Baseline Analysis and Tool Setup**
- Configure coverage tools for all languages in the project
- Establish baseline coverage metrics and thresholds
- Identify critical business logic and high-risk code paths
- Set up coverage reporting and integration with CI/CD
- Define coverage quality standards and best practices

**Framework-Specific Coverage Tools:**

**JavaScript/TypeScript Coverage:**
```json
{
  "jest": {
    "collectCoverage": true,
    "coverageDirectory": "coverage",
    "coverageReporters": ["text", "lcov", "html", "json"],
    "collectCoverageFrom": [
      "src/**/*.{js,ts}",
      "!src/**/*.test.{js,ts}",
      "!src/**/*.spec.{js,ts}",
      "!src/**/*.d.ts"
    ],
    "coverageThreshold": {
      "global": {
        "branches": 80,
        "functions": 85,
        "lines": 80,
        "statements": 80
      },
      "src/core/": {
        "branches": 90,
        "functions": 95,
        "lines": 90,
        "statements": 90
      }
    }
  }
}
```

**Python Coverage:**
```ini
[run]
source = src
omit = 
    */tests/*
    */test_*
    */conftest.py
    */migrations/*
    */venv/*
    */env/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

[html]
directory = htmlcov

[xml]
output = coverage.xml
```

**Go Coverage:**
```bash
# Generate coverage profile
go test -coverprofile=coverage.out -covermode=atomic ./...

# Generate HTML report
go tool cover -html=coverage.out -o coverage.html

# Generate func coverage
go tool cover -func=coverage.out

# Coverage with race detection
go test -race -coverprofile=coverage.out -covermode=atomic ./...
```

**Step 1: Comprehensive Coverage Analysis**

**Coverage Analysis Framework:**
```typescript
interface CoverageAnalysis {
  overall_metrics: {
    line_coverage: number;
    branch_coverage: number;
    function_coverage: number;
    statement_coverage: number;
  };
  file_coverage: FileCoverage[];
  critical_gaps: CriticalGap[];
  priority_areas: PriorityArea[];
  coverage_trends: CoverageTrend[];
}

interface FileCoverage {
  file_path: string;
  coverage_percentage: number;
  uncovered_lines: number[];
  uncovered_functions: string[];
  uncovered_branches: BranchInfo[];
  complexity_score: number;
  business_impact: 'high' | 'medium' | 'low';
  test_difficulty: 'easy' | 'medium' | 'hard';
}

interface CriticalGap {
  file_path: string;
  function_name: string;
  gap_type: 'missing_unit_test' | 'missing_integration_test' | 'missing_edge_case' | 'missing_error_path';
  business_impact: 'critical' | 'high' | 'medium' | 'low';
  complexity: number;
  estimated_effort: number;
  dependencies: string[];
}

class CoverageAnalyzer {
  async analyzeCoverage(projectPath: string): Promise<CoverageAnalysis> {
    // Start comprehensive coverage analysis
    
    // Generate coverage data
    const coverageData = await this.generateCoverageData(projectPath);
    
    // Analyze file-level coverage
    const fileCoverage = await this.analyzeFileCoverage(coverageData);
    
    // Identify critical gaps
    const criticalGaps = await this.identifyCriticalGaps(fileCoverage);
    
    // Prioritize areas for improvement
    const priorityAreas = await this.prioritizeAreas(criticalGaps);
    
    // Analyze coverage trends
    const coverageTrends = await this.analyzeCoverageTrends(projectPath);
    
    return {
      overall_metrics: this.calculateOverallMetrics(coverageData),
      file_coverage: fileCoverage,
      critical_gaps: criticalGaps,
      priority_areas: priorityAreas,
      coverage_trends: coverageTrends
    };
  }
  
  private async generateCoverageData(projectPath: string): Promise<any> {
    const coverageCommands = {
      javascript: 'npm test -- --coverage --json',
      typescript: 'npm test -- --coverage --json',
      python: 'coverage run -m pytest && coverage json',
      go: 'go test -coverprofile=coverage.out -json ./...',
      java: 'mvn test jacoco:report',
      csharp: 'dotnet test --collect:"XPlat Code Coverage"'
    };
    
    const language = await this.detectProjectLanguage(projectPath);
    const command = coverageCommands[language];
    
    if (!command) {
      throw new Error(`Unsupported language for coverage: ${language}`);
    }
    
    return await this.executeCoverageCommand(command, projectPath);
  }
  
  private async identifyCriticalGaps(fileCoverage: FileCoverage[]): Promise<CriticalGap[]> {
    const gaps = [];
    
    for (const file of fileCoverage) {
      // Analyze uncovered functions
      for (const func of file.uncovered_functions) {
        const functionInfo = await this.analyzeFunctionComplexity(file.file_path, func);
        
        gaps.push({
          file_path: file.file_path,
          function_name: func,
          gap_type: 'missing_unit_test',
          business_impact: this.assessBusinessImpact(file.file_path, func),
          complexity: functionInfo.complexity,
          estimated_effort: this.estimateTestEffort(functionInfo),
          dependencies: functionInfo.dependencies
        });
      }
      
      // Analyze uncovered branches (error paths)
      for (const branch of file.uncovered_branches) {
        if (branch.is_error_path) {
          gaps.push({
            file_path: file.file_path,
            function_name: branch.function_name,
            gap_type: 'missing_error_path',
            business_impact: 'high',
            complexity: branch.complexity,
            estimated_effort: 2,
            dependencies: []
          });
        }
      }
    }
    
    return gaps.sort((a, b) => this.prioritizeGap(a) - this.prioritizeGap(b));
  }
  
  private assessBusinessImpact(filePath: string, functionName: string): 'critical' | 'high' | 'medium' | 'low' {
    const highImpactPatterns = [
      /payment/i,
      /auth/i,
      /security/i,
      /user/i,
      /order/i,
      /transaction/i,
      /billing/i,
      /api/i,
      /core/i,
      /critical/i
    ];
    
    const mediumImpactPatterns = [
      /validation/i,
      /service/i,
      /manager/i,
      /controller/i,
      /handler/i,
      /processor/i
    ];
    
    const fullPath = `${filePath}:${functionName}`;
    
    if (highImpactPatterns.some(pattern => pattern.test(fullPath))) {
      return 'critical';
    }
    
    if (mediumImpactPatterns.some(pattern => pattern.test(fullPath))) {
      return 'high';
    }
    
    return 'medium';
  }
}
```

**Step 2: Parallel Agent Deployment for Coverage Improvement**

**Agent Spawning Strategy:**
```
"I've identified 47 critical coverage gaps across 12 modules. I'll spawn specialized agents:

1. **Core Business Logic Agent**: 'Implement tests for authentication, payment, and user management'
2. **API Layer Agent**: 'Create tests for REST endpoints and GraphQL resolvers'
3. **Data Layer Agent**: 'Test database operations, models, and data validation'
4. **Error Handling Agent**: 'Implement tests for error paths and exception handling'
5. **Edge Case Agent**: 'Test boundary conditions, null values, and edge scenarios'
6. **Performance Agent**: 'Add benchmark tests for performance-critical functions'
7. **Integration Agent**: 'Create tests for service interactions and external APIs'
8. **Security Agent**: 'Test authentication, authorization, and security features'

Each agent will focus on their domain while coordinating to avoid test conflicts."
```

**Step 3: Critical Path Test Implementation**

**Test Generation Framework:**
```typescript
class TestGenerator {
  async generateTestsForGaps(gaps: CriticalGap[]): Promise<GeneratedTest[]> {
    const generatedTests = [];
    
    for (const gap of gaps) {
      // Generate tests for the gap
      
      const testCode = await this.generateTestCode(gap);
      const testFilePath = this.getTestFilePath(gap.file_path);
      
      generatedTests.push({
        test_file_path: testFilePath,
        test_code: testCode,
        gap_info: gap,
        estimated_coverage_increase: this.estimateCoverageIncrease(gap)
      });
    }
    
    return generatedTests;
  }
  
  private async generateTestCode(gap: CriticalGap): Promise<string> {
    const functionInfo = await this.analyzeFunctionSignature(gap.file_path, gap.function_name);
    
    switch (gap.gap_type) {
      case 'missing_unit_test':
        return this.generateUnitTest(functionInfo);
      case 'missing_integration_test':
        return this.generateIntegrationTest(functionInfo);
      case 'missing_edge_case':
        return this.generateEdgeCaseTest(functionInfo);
      case 'missing_error_path':
        return this.generateErrorPathTest(functionInfo);
      default:
        throw new Error(`Unknown gap type: ${gap.gap_type}`);
    }
  }
  
  private generateUnitTest(functionInfo: FunctionInfo): string {
    const testTemplate = this.getTestTemplate(functionInfo.language);
    
    return testTemplate.replace('{{FUNCTION_NAME}}', functionInfo.name)
      .replace('{{TEST_CASES}}', this.generateTestCases(functionInfo))
      .replace('{{IMPORTS}}', this.generateImports(functionInfo))
      .replace('{{SETUP}}', this.generateSetup(functionInfo))
      .replace('{{TEARDOWN}}', this.generateTeardown(functionInfo));
  }
  
  private generateTestCases(functionInfo: FunctionInfo): string {
    const testCases = [];
    
    // Happy path tests
    testCases.push(this.generateHappyPathTest(functionInfo));
    
    // Edge case tests
    testCases.push(...this.generateEdgeCaseTests(functionInfo));
    
    // Error path tests
    testCases.push(...this.generateErrorPathTests(functionInfo));
    
    // Boundary tests
    testCases.push(...this.generateBoundaryTests(functionInfo));
    
    return testCases.join('\n\n');
  }
  
  private generateHappyPathTest(functionInfo: FunctionInfo): string {
    const validInputs = this.generateValidInputs(functionInfo.parameters);
    const expectedOutput = this.generateExpectedOutput(functionInfo.return_type);
    
    return `
  test('${functionInfo.name} should handle valid inputs correctly', async () => {
    // Arrange
    ${this.generateArrangeCode(validInputs)}
    
    // Act
    const result = await ${functionInfo.name}(${validInputs.join(', ')});
    
    // Assert
    expect(result).${this.generateAssertionCode(expectedOutput)};
  });`;
  }
  
  private generateEdgeCaseTests(functionInfo: FunctionInfo): string[] {
    const edgeCases = [];
    
    // Null/undefined tests
    if (functionInfo.parameters.some(p => p.nullable)) {
      edgeCases.push(`
  test('${functionInfo.name} should handle null inputs', async () => {
    // Test null handling
    await expect(${functionInfo.name}(null)).${this.generateNullAssertionCode(functionInfo)};
  });`);
    }
    
    // Empty string/array tests
    if (functionInfo.parameters.some(p => p.type === 'string' || p.type === 'array')) {
      edgeCases.push(`
  test('${functionInfo.name} should handle empty inputs', async () => {
    // Test empty input handling
    const result = await ${functionInfo.name}('');
    expect(result).${this.generateEmptyAssertionCode(functionInfo)};
  });`);
    }
    
    // Boundary value tests
    if (functionInfo.parameters.some(p => p.type === 'number')) {
      edgeCases.push(`
  test('${functionInfo.name} should handle boundary values', async () => {
    // Test boundary values
    await expect(${functionInfo.name}(Number.MAX_VALUE)).${this.generateBoundaryAssertionCode(functionInfo)};
    await expect(${functionInfo.name}(Number.MIN_VALUE)).${this.generateBoundaryAssertionCode(functionInfo)};
  });`);
    }
    
    return edgeCases;
  }
  
  private generateErrorPathTests(functionInfo: FunctionInfo): string[] {
    const errorTests = [];
    
    // Invalid input tests
    errorTests.push(`
  test('${functionInfo.name} should handle invalid inputs', async () => {
    // Test invalid input handling
    await expect(${functionInfo.name}(${this.generateInvalidInputs(functionInfo)}))
      .rejects.toThrow('${this.generateExpectedErrorMessage(functionInfo)}');
  });`);
    
    // Permission/authorization tests
    if (functionInfo.requires_auth) {
      errorTests.push(`
  test('${functionInfo.name} should handle unauthorized access', async () => {
    // Test unauthorized access
    await expect(${functionInfo.name}(validInput, { user: null }))
      .rejects.toThrow('Unauthorized');
  });`);
    }
    
    // External dependency failure tests
    if (functionInfo.external_dependencies.length > 0) {
      errorTests.push(`
  test('${functionInfo.name} should handle external service failures', async () => {
    // Mock external service failure
    jest.spyOn(${functionInfo.external_dependencies[0]}, 'call').mockRejectedValue(new Error('Service unavailable'));
    
    await expect(${functionInfo.name}(validInput))
      .rejects.toThrow('Service unavailable');
  });`);
    }
    
    return errorTests;
  }
}
```

**Step 4: Advanced Coverage Metrics and Analysis**

**Coverage Quality Assessment:**
```typescript
class CoverageQualityAssessor {
  async assessCoverageQuality(coverageData: any): Promise<CoverageQualityReport> {
    // Assess coverage quality
    
    const qualityMetrics = {
      test_effectiveness: await this.measureTestEffectiveness(coverageData),
      mutation_testing_score: await this.runMutationTesting(coverageData),
      branch_coverage_quality: await this.analyzeBranchCoverage(coverageData),
      assertion_quality: await this.analyzeAssertionQuality(coverageData),
      test_maintainability: await this.assessTestMaintainability(coverageData)
    };
    
    return {
      overall_quality_score: this.calculateOverallQualityScore(qualityMetrics),
      quality_metrics: qualityMetrics,
      improvement_recommendations: this.generateQualityRecommendations(qualityMetrics),
      coverage_gaps: await this.identifyQualityGaps(qualityMetrics)
    };
  }
  
  private async measureTestEffectiveness(coverageData: any): Promise<number> {
    // Measure how well tests catch bugs
    const testFiles = await this.findTestFiles(coverageData.project_path);
    let effectivenessScore = 0;
    
    for (const testFile of testFiles) {
      const testAnalysis = await this.analyzeTestFile(testFile);
      
      // Check for meaningful assertions
      const assertionQuality = this.assessAssertions(testAnalysis.assertions);
      
      // Check for comprehensive test scenarios
      const scenarioCoverage = this.assessScenarioCoverage(testAnalysis.test_cases);
      
      // Check for proper mocking
      const mockingQuality = this.assessMockingQuality(testAnalysis.mocks);
      
      effectivenessScore += (assertionQuality + scenarioCoverage + mockingQuality) / 3;
    }
    
    return effectivenessScore / testFiles.length;
  }
  
  private async runMutationTesting(coverageData: any): Promise<number> {
    // Use mutation testing to assess test quality
    const mutationTools = {
      javascript: 'stryker',
      typescript: 'stryker',
      python: 'mutmut',
      java: 'pitest',
      csharp: 'stryker-net'
    };
    
    const language = await this.detectLanguage(coverageData.project_path);
    const mutationTool = mutationTools[language];
    
    if (!mutationTool) {
      // Mutation testing not available for this language
      return 0;
    }
    
    try {
      const mutationResult = await this.executeMutationTesting(mutationTool, coverageData.project_path);
      return mutationResult.mutation_score;
    } catch (error) {
      // Mutation testing failed
      return 0;
    }
  }
  
  private async analyzeBranchCoverage(coverageData: any): Promise<BranchCoverageAnalysis> {
    const branchAnalysis = {
      total_branches: 0,
      covered_branches: 0,
      uncovered_error_paths: 0,
      uncovered_edge_cases: 0,
      critical_uncovered_branches: []
    };
    
    for (const file of coverageData.files) {
      const ast = await this.parseFileAST(file.path);
      const branches = this.extractBranches(ast);
      
      branchAnalysis.total_branches += branches.length;
      
      for (const branch of branches) {
        if (this.isBranchCovered(branch, file.coverage)) {
          branchAnalysis.covered_branches++;
        } else {
          if (branch.is_error_path) {
            branchAnalysis.uncovered_error_paths++;
          }
          
          if (branch.is_edge_case) {
            branchAnalysis.uncovered_edge_cases++;
          }
          
          if (branch.is_critical) {
            branchAnalysis.critical_uncovered_branches.push(branch);
          }
        }
      }
    }
    
    return branchAnalysis;
  }
}
```

**Step 5: Performance Benchmarking for Critical Paths**

**Performance Test Generation:**
```typescript
class PerformanceBenchmarkGenerator {
  async generatePerformanceTests(criticalPaths: CriticalPath[]): Promise<PerformanceTest[]> {
    const performanceTests = [];
    
    for (const path of criticalPaths) {
      // Generate performance tests for critical path
      
      const benchmarkTest = await this.generateBenchmarkTest(path);
      const loadTest = await this.generateLoadTest(path);
      const memoryTest = await this.generateMemoryTest(path);
      
      performanceTests.push({
        function_name: path.function_name,
        benchmark_test: benchmarkTest,
        load_test: loadTest,
        memory_test: memoryTest,
        performance_targets: this.definePerformanceTargets(path)
      });
    }
    
    return performanceTests;
  }
  
  private async generateBenchmarkTest(path: CriticalPath): Promise<string> {
    const language = path.language;
    
    switch (language) {
      case 'javascript':
      case 'typescript':
        return `
describe('${path.function_name} Performance', () => {
  test('should execute within performance targets', async () => {
    const iterations = 1000;
    const start = performance.now();
    
    for (let i = 0; i < iterations; i++) {
      await ${path.function_name}(${this.generateBenchmarkInputs(path)});
    }
    
    const end = performance.now();
    const averageTime = (end - start) / iterations;
    
    expect(averageTime).toBeLessThan(${path.performance_targets.max_execution_time});
  });
  
  test('should handle concurrent executions', async () => {
    const concurrentCalls = 10;
    const start = performance.now();
    
    const promises = Array.from({ length: concurrentCalls }, () => 
      ${path.function_name}(${this.generateBenchmarkInputs(path)})
    );
    
    await Promise.all(promises);
    
    const end = performance.now();
    const totalTime = end - start;
    
    expect(totalTime).toBeLessThan(${path.performance_targets.max_concurrent_time});
  });
});`;
        
      case 'go':
        return `
func Benchmark${path.function_name}(b *testing.B) {
    input := ${this.generateGoInputs(path)}
    
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        ${path.function_name}(input)
    }
}

func Benchmark${path.function_name}Parallel(b *testing.B) {
    input := ${this.generateGoInputs(path)}
    
    b.ResetTimer()
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            ${path.function_name}(input)
        }
    })
}`;
        
      case 'python':
        return `
import pytest
import time
from memory_profiler import profile

class TestPerformance${path.function_name}:
    def test_execution_time(self):
        """Test execution time within targets"""
        iterations = 1000
        start_time = time.time()
        
        for _ in range(iterations):
            ${path.function_name}(${this.generatePythonInputs(path)})
        
        end_time = time.time()
        average_time = (end_time - start_time) / iterations
        
        assert average_time < ${path.performance_targets.max_execution_time}
    
    @profile
    def test_memory_usage(self):
        """Test memory usage within targets"""
        result = ${path.function_name}(${this.generatePythonInputs(path)})
        # Memory profile will be generated automatically
        
    def test_concurrent_execution(self):
        """Test concurrent execution performance"""
        import concurrent.futures
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [
                executor.submit(${path.function_name}, ${this.generatePythonInputs(path)})
                for _ in range(10)
            ]
            
            start_time = time.time()
            results = [future.result() for future in futures]
            end_time = time.time()
            
            assert end_time - start_time < ${path.performance_targets.max_concurrent_time}
`;
      
      default:
        throw new Error(`Unsupported language for performance testing: ${language}`);
    }
  }
}
```

**Step 6: Coverage Report Generation and Visualization**

**Comprehensive Coverage Reporting:**
```typescript
class CoverageReporter {
  async generateComprehensiveReport(analysis: CoverageAnalysis): Promise<CoverageReport> {
    // Generate comprehensive coverage report
    
    const report = {
      executive_summary: this.generateExecutiveSummary(analysis),
      detailed_metrics: this.generateDetailedMetrics(analysis),
      critical_gaps: this.formatCriticalGaps(analysis.critical_gaps),
      priority_recommendations: this.generatePriorityRecommendations(analysis),
      coverage_trends: this.formatCoverageTrends(analysis.coverage_trends),
      quality_assessment: await this.assessCoverageQuality(analysis),
      actionable_items: this.generateActionableItems(analysis)
    };
    
    // Generate different report formats
    await this.generateHTMLReport(report);
    await this.generateMarkdownReport(report);
    await this.generateJSONReport(report);
    await this.generateSlackReport(report);
    
    return report;
  }
  
  private generateExecutiveSummary(analysis: CoverageAnalysis): ExecutiveSummary {
    const criticalGaps = analysis.critical_gaps.filter(gap => gap.business_impact === 'critical');
    const highPriorityGaps = analysis.critical_gaps.filter(gap => gap.business_impact === 'high');
    
    return {
      overall_coverage: analysis.overall_metrics.line_coverage,
      critical_coverage_status: criticalGaps.length === 0 ? 'GOOD' : 'NEEDS_ATTENTION',
      high_priority_gaps: highPriorityGaps.length,
      estimated_effort: this.calculateTotalEffort(analysis.critical_gaps),
      key_recommendations: this.getTopRecommendations(analysis, 5),
      coverage_trend: this.getCoverageTrend(analysis.coverage_trends)
    };
  }
  
  private generateDetailedMetrics(analysis: CoverageAnalysis): DetailedMetrics {
    return {
      coverage_by_module: this.calculateModuleCoverage(analysis.file_coverage),
      coverage_by_complexity: this.calculateComplexityCoverage(analysis.file_coverage),
      coverage_by_business_impact: this.calculateBusinessImpactCoverage(analysis.file_coverage),
      test_type_distribution: this.calculateTestTypeDistribution(analysis),
      coverage_debt: this.calculateCoverageDebt(analysis.critical_gaps)
    };
  }
  
  private generateActionableItems(analysis: CoverageAnalysis): ActionableItem[] {
    const items = [];
    
    // High-impact, low-effort items
    const quickWins = analysis.critical_gaps.filter(gap => 
      gap.business_impact === 'high' && gap.estimated_effort <= 2
    );
    
    for (const gap of quickWins) {
      items.push({
        priority: 'HIGH',
        type: 'QUICK_WIN',
        title: `Add tests for ${gap.function_name}`,
        description: `Critical function with ${gap.business_impact} business impact`,
        estimated_effort: gap.estimated_effort,
        file_path: gap.file_path,
        acceptance_criteria: this.generateAcceptanceCriteria(gap)
      });
    }
    
    // Critical coverage gaps
    const criticalGaps = analysis.critical_gaps.filter(gap => 
      gap.business_impact === 'critical'
    );
    
    for (const gap of criticalGaps) {
      items.push({
        priority: 'CRITICAL',
        type: 'COVERAGE_GAP',
        title: `Critical coverage gap: ${gap.function_name}`,
        description: `Critical business logic without adequate test coverage`,
        estimated_effort: gap.estimated_effort,
        file_path: gap.file_path,
        acceptance_criteria: this.generateAcceptanceCriteria(gap)
      });
    }
    
    return items.sort((a, b) => this.prioritizeItem(a) - this.prioritizeItem(b));
  }
  
  private async generateHTMLReport(report: CoverageReport): Promise<void> {
    const htmlTemplate = `
<!DOCTYPE html>
<html>
<head>
    <title>Coverage Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .summary { background: #f5f5f5; padding: 20px; border-radius: 5px; }
        .metric { display: inline-block; margin: 10px; padding: 10px; background: white; border-radius: 3px; }
        .critical { color: #d32f2f; }
        .high { color: #f57c00; }
        .medium { color: #1976d2; }
        .low { color: #388e3c; }
        .chart { margin: 20px 0; }
        .gap-item { margin: 10px 0; padding: 10px; border-left: 4px solid #ddd; }
        .gap-critical { border-left-color: #d32f2f; }
        .gap-high { border-left-color: #f57c00; }
    </style>
</head>
<body>
    <h1>Test Coverage Report</h1>
    
    <div class="summary">
        <h2>Executive Summary</h2>
        <div class="metric">
            <strong>Overall Coverage:</strong> ${report.executive_summary.overall_coverage}%
        </div>
        <div class="metric">
            <strong>Critical Status:</strong> ${report.executive_summary.critical_coverage_status}
        </div>
        <div class="metric">
            <strong>High Priority Gaps:</strong> ${report.executive_summary.high_priority_gaps}
        </div>
        <div class="metric">
            <strong>Estimated Effort:</strong> ${report.executive_summary.estimated_effort} hours
        </div>
    </div>
    
    <div class="detailed-metrics">
        <h2>Detailed Metrics</h2>
        ${this.generateMetricsHTML(report.detailed_metrics)}
    </div>
    
    <div class="critical-gaps">
        <h2>Critical Coverage Gaps</h2>
        ${this.generateGapsHTML(report.critical_gaps)}
    </div>
    
    <div class="actionable-items">
        <h2>Actionable Items</h2>
        ${this.generateActionableItemsHTML(report.actionable_items)}
    </div>
    
    <div class="chart">
        <h2>Coverage Visualization</h2>
        <canvas id="coverageChart"></canvas>
    </div>
    
    <script>
        ${this.generateChartScript(report)}
    </script>
</body>
</html>`;
    
    await fs.writeFile('coverage-report.html', htmlTemplate);
    // HTML coverage report generated: coverage-report.html
  }
}
```

**Step 7: Automated Test Implementation**

**Test Implementation Engine:**
```typescript
class TestImplementationEngine {
  async implementMissingTests(gaps: CriticalGap[]): Promise<TestImplementationResult[]> {
    // Implement missing tests
    
    const results = [];
    
    for (const gap of gaps) {
      try {
        // Implement test for the gap
        
        // Generate test code
        const testCode = await this.generateTestCode(gap);
        
        // Determine test file location
        const testFilePath = this.getTestFilePath(gap.file_path);
        
        // Write or update test file
        await this.writeTestFile(testFilePath, testCode, gap);
        
        // Validate test implementation
        const validation = await this.validateTestImplementation(testFilePath, gap);
        
        results.push({
          gap: gap,
          test_file_path: testFilePath,
          implementation_status: validation.is_valid ? 'SUCCESS' : 'FAILED',
          coverage_improvement: validation.coverage_improvement,
          issues: validation.issues
        });
        
        // Test implemented successfully
        
      } catch (error) {
        results.push({
          gap: gap,
          test_file_path: null,
          implementation_status: 'ERROR',
          error: error.message
        });
        
        // Failed to implement test for gap
      }
    }
    
    return results;
  }
  
  private async validateTestImplementation(testFilePath: string, gap: CriticalGap): Promise<TestValidation> {
    // Run the new test
    const testResult = await this.runTest(testFilePath);
    
    // Measure coverage improvement
    const coverageImprovement = await this.measureCoverageImprovement(gap.file_path, testFilePath);
    
    // Validate test quality
    const qualityCheck = await this.validateTestQuality(testFilePath);
    
    return {
      is_valid: testResult.passed && qualityCheck.is_high_quality,
      coverage_improvement: coverageImprovement,
      issues: [
        ...testResult.issues,
        ...qualityCheck.issues
      ]
    };
  }
}
```

**Coverage Improvement Quality Checklist:**
- [ ] Critical business logic has comprehensive test coverage
- [ ] Coverage targets are achieved with meaningful tests
- [ ] Error paths and edge cases are thoroughly tested
- [ ] Performance-critical paths have benchmark tests
- [ ] Test quality is maintained while improving coverage
- [ ] Coverage reports provide actionable insights
- [ ] Test implementation is automated and repeatable
- [ ] Coverage trends show continuous improvement

**Agent Coordination for Large Codebases:**
```
"For comprehensive coverage improvement, I'll coordinate multiple specialized agents:

Primary Coverage Agent: Overall coverage analysis and coordination
‚îú‚îÄ‚îÄ Analysis Agent: Analyze current coverage and identify gaps
‚îú‚îÄ‚îÄ Unit Test Agent: Implement missing unit tests
‚îú‚îÄ‚îÄ Integration Test Agent: Create integration tests for system interactions
‚îú‚îÄ‚îÄ Edge Case Agent: Test boundary conditions and error scenarios
‚îú‚îÄ‚îÄ Performance Agent: Add performance benchmarks for critical paths
‚îú‚îÄ‚îÄ Quality Agent: Ensure test quality and effectiveness
‚îî‚îÄ‚îÄ Report Agent: Generate comprehensive coverage reports and metrics

Each agent will focus on their domain while coordinating to achieve comprehensive coverage improvement."
```

**Anti-Patterns to Avoid:**
- ‚ùå **Confirmation testing: Reading uncovered code then writing tests that verify what it does** (tests pass by construction, miss real bugs)
- ‚ùå **Treating line coverage as the goal instead of specification coverage** (high coverage, low confidence)
- ‚ùå Writing tests just for coverage numbers (meaningless tests)
- ‚ùå Ignoring complex or difficult-to-test code (coverage gaps)
- ‚ùå Focusing only on line coverage (missing branch coverage)
- ‚ùå Skipping error paths and edge cases (poor test quality)
- ‚ùå Not measuring test effectiveness (low-quality coverage)
- ‚ùå Ignoring performance testing for critical paths (performance regressions)

**Final Verification:**
Before completing coverage improvement:
- Are all critical consumer-facing behaviors specified in tests?
- Do tests define expected behavior, not just confirm existing code?
- Would tests fail if features break in real consumer usage?
- Are error scenarios and edge cases specified from consumer perspective?
- Are performance expectations for critical paths defined in tests?
- Do coverage reports identify unspecified behaviors, not just uncovered lines?
- Is test quality maintained while improving specification coverage?

**Final Commitment:**
- **I will**: Identify unspecified consumer-facing behaviors, not just uncovered code lines
- **I will**: Write test specifications WITHOUT reading implementation code first
- **I will**: Use multiple agents to create behavior specifications in parallel
- **I will**: Focus on specification coverage that defines expected behavior
- **I will**: Generate reports identifying unspecified behaviors and consumer gaps
- **I will NOT**: Write confirmation tests that verify what code already does
- **I will NOT**: Treat line coverage as the primary success metric
- **I will NOT**: Read implementation before specifying expected behavior
- **I will NOT**: Skip complex or difficult-to-test code
- **I will NOT**: Ignore consumer-facing error paths and edge cases

**REMEMBER:**
This is SPECIFICATION-FIRST COVERAGE IMPROVEMENT mode - identify unspecified consumer behaviors, write test specifications BEFORE reading implementation, and achieve specification coverage that defines expected behavior. Coverage tools identify gaps in specifications, not goals to achieve through confirmation testing.

**Coverage is an INDICATOR of missing specifications, not a goal achieved by confirming existing code.**

Executing specification-first coverage improvement protocol for behavior-driven validation...